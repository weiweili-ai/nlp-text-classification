{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880a776e",
   "metadata": {},
   "source": [
    "Text classification is one of the most common tasks in NLP. It is the process of assigning a label or category to a given piece of text. For example, we can classify emails as spam or not spam, tweets as positive or negative, and articles as relevant or not relevant to a given topic.\n",
    "\n",
    "For example, classify users comments on SQO as relevant to price, competitor, product upgrade or other-reasons(such as client budget). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df210e",
   "metadata": {},
   "source": [
    "💥 Transformer是一种基于自注意力机制的深度学习模型，广泛应用于自然语言处理（NLP）任务。\n",
    "\t\n",
    "💥 与传统的循环神经网络（RNN）不同，Transformer不依赖于序列的顺序处理，而是通过自注意力机制并行处理输入数据的所有位置，捕捉长距离依赖关系。\n",
    "\t\n",
    "👉 Transformer的核心组件包括编码器（Encoder）和解码器（Decoder），每个组件都由多层自注意力和前馈神经网络组成。\n",
    "\t\n",
    "👉 这种架构使得Transformer在处理大规模数据时，能更高效地并行计算，从而大大提高了训练速度和性能，成为了包括BERT、GPT等在内的许多先进模型的基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf31e6",
   "metadata": {},
   "source": [
    "# 使用PyTorch实现简单文本分类¶\n",
    "一、准备工作：环境配置，加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bf8cb",
   "metadata": {},
   "source": [
    "# pip install spacy\n",
    "spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.\n",
    "Unlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage.spaCy also supports deep learning workflows that allow connecting statistical models trained by popular machine learning libraries like TensorFlow, PyTorch or MXNet through its own machine learning library Thinc.\n",
    "Using Thinc as its backend, spaCy features convolutional neural network models for part-of-speech tagging, dependency parsing, text categorization and named entity recognition (NER). Prebuilt statistical neural network models perform these tasks are available for 23 languages, including English, Portuguese, Spanish, Russian and Chinese, and there is also a multi-language NER model. Additional support for tokenization for more than 65 languages allows users to train custom models on their own datasets as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee113e60",
   "metadata": {},
   "source": [
    "# pip install nltk\n",
    "https://www.nltk.org/howto/wordnet.html\n",
    "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. \n",
    "It supports classification, tokenization(标记化), stemming（词干提取）, tagging（标记）, parsing（解析）, and semantic reasoning（语义推理） functionalities. \n",
    "NLTK includes graphical demonstrations（图形演示） and sample data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dafef2c",
   "metadata": {},
   "source": [
    "🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. AutoTokenizer is a special class in the Huggingface Transformers library. It helps you choose the right tokenizer for your model without knowing the details.\n",
    "\n",
    "These models can be applied on:\n",
    "\n",
    "📝 Text, for tasks like text classification, information extraction, question answering, summarization, translation, and text generation, in over 100 languages.\n",
    "🖼️ Images, for tasks like image classification, object detection, and segmentation.\n",
    "🗣️ Audio, for tasks like speech recognition and audio classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef1e6c3",
   "metadata": {},
   "source": [
    "# pip install transformers\n",
    "Tokenizers are essential tools in machine learning, especially in natural language processing (NLP). They break down text into smaller units called tokens.\n",
    "\n",
    "These tokens can be words, subwords, or characters.\n",
    "\n",
    "For example, let's take the sentence \"I love apples.\" When we tokenize this sentence, it breaks down into three tokens: \"I,\" \"love,\" and \"apples.\"\n",
    "\n",
    "Tokenization makes it easier for computers to understand and process the text. It’s used for tasks like translation, sentiment analysis, and all of NLP.\n",
    "\n",
    "AutoTokenizer is a special class in the Huggingface Transformers library. It helps you choose the right tokenizer for your model without knowing the details.\n",
    "\n",
    "Think of it as a smart assistant that knows which tool to use for the job.\n",
    "\n",
    "The AutoTokenizer is easy to use. You don’t have to remember which tokenizer goes with which model. It ensures you use the correct tokenizer for the model, reducing errors and improving consistency.\n",
    "\n",
    "Autotokenizer is flexible. It works with many different models, allowing you to switch models without changing much code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab3d542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "3.8.3\n",
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string,time\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "print(torch.__version__)\n",
    "print(spacy.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05be6bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/georgeli/nltk_data',\n",
       " '/Users/georgeli/anaconda3/nltk_data',\n",
       " '/Users/georgeli/anaconda3/share/nltk_data',\n",
       " '/Users/georgeli/anaconda3/lib/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/local/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/local/lib/nltk_data']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544a5689",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/georgeli/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/georgeli/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/georgeli/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /Users/georgeli/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32128e5d",
   "metadata": {},
   "source": [
    "Look up a word using synsets(); this function has an optional pos argument which lets you constrain the part of speech of the word. 使用 synsets() 查找单词；这个函数有一个可选的 pos 参数，它可以让你限制单词的词性："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2924bed",
   "metadata": {},
   "source": [
    "WordNet: an NLTK interface（接口)for WordNet. \n",
    "WordNet is a lexical database（词汇数据库） of English. \n",
    "Using synsets（同义词集）, helps find conceptual relationships between words such as hypernyms（上位词）, hyponyms（下位词）, synonyms（同义词）, antonyms（反义词） etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8477025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('happy.a.01'), Synset('felicitous.s.02'), Synset('glad.s.02'), Synset('happy.s.04')]\n",
      "[Synset('chase.v.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# Synset: a set of synonyms that share a common meaning.\n",
    "print(wordnet.synsets('happy'))\n",
    "print(wordnet.synsets('dog', pos=wordnet.VERB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28fd1e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go after with the intent to catch\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.synset('chase.v.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50d6aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dog barked all night\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.synset('dog.n.01').examples()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3e62cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['いぬ', 'まわし者', 'スパイ', '回し者', '回者', '密偵', '工作員', '廻し者', '廻者', '探', '探り', '犬', '秘密捜査員', '諜報員', '諜者', '間者', '間諜', '隠密']\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.synset('spy.n.01').lemma_names('jpn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "441edd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['auto', 'automobile', 'machine', 'motorcar'],\n",
       " ['railcar', 'railroad_car', 'railway_car'],\n",
       " ['gondola'],\n",
       " ['elevator_car'],\n",
       " ['cable_car']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The synonyms of a word are returned as a nested list of synonyms of the different senses of the input word in the given language, since these different senses are not mutual synonyms:\n",
    "wordnet.synonyms('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1e400",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08bc4874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Household</td>\n",
       "      <td>Paper Plane Design Framed Wall Hanging Motivat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Household</td>\n",
       "      <td>SAF 'Floral' Framed Painting (Wood, 30 inch x ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Household</td>\n",
       "      <td>SAF 'UV Textured Modern Art Print Framed' Pain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Household</td>\n",
       "      <td>SAF Flower Print Framed Painting (Synthetic, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Household</td>\n",
       "      <td>Incredible Gifts India Wooden Happy Birthday U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     segment                                               text\n",
       "0  Household  Paper Plane Design Framed Wall Hanging Motivat...\n",
       "1  Household  SAF 'Floral' Framed Painting (Wood, 30 inch x ...\n",
       "2  Household  SAF 'UV Textured Modern Art Print Framed' Pain...\n",
       "3  Household  SAF Flower Print Framed Painting (Synthetic, 1...\n",
       "4  Household  Incredible Gifts India Wooden Happy Birthday U..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"ecommerceDataset.csv\",header=None, names=[\"segment\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdf45683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50425, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3322584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "segment\n",
       "Household                 19313\n",
       "Books                     11820\n",
       "Electronics               10621\n",
       "Clothing & Accessories     8671\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.segment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d51e7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'segment'\n",
    "samples_per_class = 600\n",
    "balanced_data = df.groupby(label_column).apply(lambda x: x.sample(n=samples_per_class, random_state=42))\n",
    "balanced_data = balanced_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e372c6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   segment  2400 non-null   object\n",
      " 1   text     2400 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 37.6+ KB\n"
     ]
    }
   ],
   "source": [
    "balanced_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3e87397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, max_length=128):\n",
    "        # Bert tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") #https://huggingface.co/google-bert/bert-base-uncased\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        pattern = re.compile('<.*?>')\n",
    "        text=pattern.sub(r'', text)\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "        text=text.translate(str.maketrans('', '', string.punctuation))\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokenizer = TweetTokenizer()\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        text = ' '.join(tokens)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "        text = ' '.join(tokens)\n",
    "        return text.strip()\n",
    "    def text_to_sequence(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d81b2cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organic Shastra Pav Bhaji Masala 150g Pav bhaji, a popular street food of India, is incomplete without Pav Bhaji masala. It is a blend of spices - red chillies, coriander seeds, cumin seeds, black pepper, cinnamon, clove, black cardamom, dry mango powder, fennel seeds and turmeric powder. Not just in pav bhaji, this aromatic masala can also be used to spice up curries, stir fries, rice preparations etc\n",
      "**************************************************\n",
      "organic shastra pav bhaji masala 150g pav bhaji popular street food india incomplete without pav bhaji masala blend spice red chilli coriander seed cumin seed black pepper cinnamon clove black cardamom dry mango powder fennel seed turmeric powder pav bhaji aromatic masala also used spice curry stir fry rice preparation etc\n"
     ]
    }
   ],
   "source": [
    "prosesor=TextPreprocessor()\n",
    "test=balanced_data['text'][1]\n",
    "print(test)\n",
    "print('*'*50)\n",
    "print(prosesor.clean_text(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "191ea096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, preprocessor=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.preprocessor.text_to_sequence(text)\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'],\n",
    "            'attention_mask': encoding['attention_mask'],\n",
    "            'length': torch.sum(encoding['attention_mask'])\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['label'] = torch.tensor(self.labels[idx])\n",
    "            \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f538c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Books', 'Clothing & Accessories', 'Electronics', 'Household']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1c19c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {category: idx for idx, category in enumerate(categories)}\n",
    "balanced_data['segment'] = balanced_data['segment'].map(category_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15571b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "texts = balanced_data.text.tolist()\n",
    "labels = balanced_data.segment.tolist()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25eee373",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,bidirectional=True)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids,attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "         # Extract the output of the last time step \n",
    "        _,(hidden,_)=self.lstm(embedded)\n",
    "        \n",
    "        # Shape Hidden:(number of layers* direction , batch,  hidden)\n",
    "        hidden_last_layer = hidden[-1,:,:]\n",
    "        hidden_last_layer = self.dropout(hidden_last_layer)\n",
    "\n",
    "        # Linear classifier layer\n",
    "        output = self.classifier(hidden_last_layer)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc2ef8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score,classification_report\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predicted_labels = []\n",
    "    all_true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='eval'):\n",
    "            input_ids= batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            _,predicted_labels=torch.max(outputs, dim =1 )\n",
    "\n",
    "\n",
    "            all_predicted_labels.extend(predicted_labels.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "    # Calculating Metrics using sklearn.metrics\n",
    "    accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n",
    "    f1= f1_score(all_true_labels,all_predicted_labels, average = 'weighted')\n",
    "    \n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83ea57e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 1/5: 100%|███████████████| 960/960 [00:46<00:00, 20.69it/s, loss=0.0003]\n",
      "eval: 100%|██████████████████████████████████| 240/240 [00:02<00:00, 109.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 , Train Loss: 0.9428, Val_Accuracy: 0.7771 Val_F1:0.7789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 2/5: 100%|███████████████| 960/960 [00:43<00:00, 22.16it/s, loss=0.0016]\n",
      "eval: 100%|██████████████████████████████████| 240/240 [00:01<00:00, 138.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 , Train Loss: 0.4076, Val_Accuracy: 0.6917 Val_F1:0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 3/5: 100%|███████████████| 960/960 [00:44<00:00, 21.66it/s, loss=0.0000]\n",
      "eval: 100%|██████████████████████████████████| 240/240 [00:02<00:00, 112.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 , Train Loss: 0.1804, Val_Accuracy: 0.8583 Val_F1:0.8576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 4/5: 100%|███████████████| 960/960 [00:44<00:00, 21.64it/s, loss=0.0000]\n",
      "eval: 100%|██████████████████████████████████| 240/240 [00:01<00:00, 133.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 , Train Loss: 0.0708, Val_Accuracy: 0.8438 Val_F1:0.8422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 5/5: 100%|███████████████| 960/960 [00:43<00:00, 22.06it/s, loss=0.0000]\n",
      "eval: 100%|██████████████████████████████████| 240/240 [00:01<00:00, 134.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 , Train Loss: 0.0387, Val_Accuracy: 0.8771 Val_F1:0.8777\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "train_dataset = SentimentDataset(train_texts, train_labels, preprocessor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "val_dataset = SentimentDataset(val_texts, val_labels, preprocessor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "\n",
    "# Model Instantiation\n",
    "vocab_size=preprocessor.vocab_size\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_labels = 4\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_labels)\n",
    "\n",
    "\n",
    "# --- Training setup ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# --- Training loop ---\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    model.train()  # Put the model in training mode\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader),desc=f\"Epoch : {epoch+1}/{epochs}\")\n",
    "    for idx, batch in progress_bar : \n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "\n",
    "        # Calculating Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        # Backward Pass & optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar loss in training\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item() / (idx + 1) :.4f}'})\n",
    "\n",
    "    avg_loss = total_loss/ len(train_dataloader)\n",
    "\n",
    "\n",
    "    # Evaluate every epoch after training.\n",
    "    val_accuracy,val_f1 = evaluate(model,val_dataloader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} , Train Loss: {avg_loss:.4f}, Val_Accuracy: {val_accuracy:.4f} Val_F1:{val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c24e346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Example\n",
    "def predict_sentiment(text, model, preprocessor, device):\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = preprocessor.text_to_sequence(text)\n",
    "    input_ids = encoding['input_ids'].unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=-1)\n",
    "\n",
    "\n",
    "    predicted_label = torch.argmax(probabilities).item()\n",
    "    return predicted_label, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c84b254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor([[0.8468, 0.0015, 0.0070, 0.1446]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(test, model, preprocessor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc57828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num={\n",
    "    'Books':0,\n",
    "    'Clothing & Accessories':1,\n",
    "    'Electronics':2,\n",
    "    'Household':3\n",
    "}\n",
    "df['segment'] = df['segment'].map(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "094c414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_row = df.sample(n=1)\n",
    "random_text = random_row['text'].values[0]\n",
    "random_Sentment = random_row['segment'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8368fd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, tensor([[1.2434e-04, 1.2626e-04, 7.5909e-04, 9.9899e-01]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(random_text, model, preprocessor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "356fef1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_Sentment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91b16c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
