{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880a776e",
   "metadata": {},
   "source": [
    "Text classification is one of the most common tasks in NLP. It is the process of assigning a label or category to a given piece of text. For example, we can classify emails as spam or not spam, tweets as positive or negative, and articles as relevant or not relevant to a given topic.\n",
    "\n",
    "For example, classify users comments on SQO as relevant to price, competitor, product upgrade or other-reasons(such as client budget). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df210e",
   "metadata": {},
   "source": [
    "ğŸ’¥ Transformeræ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ã€‚\n",
    "\t\n",
    "ğŸ’¥ ä¸ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ä¸åŒï¼ŒTransformerä¸ä¾èµ–äºåºåˆ—çš„é¡ºåºå¤„ç†ï¼Œè€Œæ˜¯é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å¹¶è¡Œå¤„ç†è¾“å…¥æ•°æ®çš„æ‰€æœ‰ä½ç½®ï¼Œæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚\n",
    "\t\n",
    "ğŸ‘‰ Transformerçš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ï¼Œæ¯ä¸ªç»„ä»¶éƒ½ç”±å¤šå±‚è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œç»„æˆã€‚\n",
    "\t\n",
    "ğŸ‘‰ è¿™ç§æ¶æ„ä½¿å¾—Transformeråœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶ï¼Œèƒ½æ›´é«˜æ•ˆåœ°å¹¶è¡Œè®¡ç®—ï¼Œä»è€Œå¤§å¤§æé«˜äº†è®­ç»ƒé€Ÿåº¦å’Œæ€§èƒ½ï¼Œæˆä¸ºäº†åŒ…æ‹¬BERTã€GPTç­‰åœ¨å†…çš„è®¸å¤šå…ˆè¿›æ¨¡å‹çš„åŸºç¡€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf31e6",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨PyTorchå®ç°ç®€å•æ–‡æœ¬åˆ†ç±»Â¶\n",
    "ä¸€ã€å‡†å¤‡å·¥ä½œï¼šç¯å¢ƒé…ç½®ï¼ŒåŠ è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bf8cb",
   "metadata": {},
   "source": [
    "# pip install spacy\n",
    "spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.\n",
    "Unlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage.spaCy also supports deep learning workflows that allow connecting statistical models trained by popular machine learning libraries like TensorFlow, PyTorch or MXNet through its own machine learning library Thinc.\n",
    "Using Thinc as its backend, spaCy features convolutional neural network models for part-of-speech tagging, dependency parsing, text categorization and named entity recognition (NER). Prebuilt statistical neural network models perform these tasks are available for 23 languages, including English, Portuguese, Spanish, Russian and Chinese, and there is also a multi-language NER model. Additional support for tokenization for more than 65 languages allows users to train custom models on their own datasets as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee113e60",
   "metadata": {},
   "source": [
    "# pip install nltk\n",
    "https://www.nltk.org/howto/wordnet.html\n",
    "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. \n",
    "It supports classification, tokenization(æ ‡è®°åŒ–), stemmingï¼ˆè¯å¹²æå–ï¼‰, taggingï¼ˆæ ‡è®°ï¼‰, parsingï¼ˆè§£æï¼‰, and semantic reasoningï¼ˆè¯­ä¹‰æ¨ç†ï¼‰ functionalities. \n",
    "NLTK includes graphical demonstrationsï¼ˆå›¾å½¢æ¼”ç¤ºï¼‰ and sample data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dafef2c",
   "metadata": {},
   "source": [
    "ğŸ¤— Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. AutoTokenizer is a special class in the Huggingface Transformers library. It helps you choose the right tokenizer for your model without knowing the details.\n",
    "\n",
    "These models can be applied on:\n",
    "\n",
    "ğŸ“ Text, for tasks like text classification, information extraction, question answering, summarization, translation, and text generation, in over 100 languages.\n",
    "ğŸ–¼ï¸ Images, for tasks like image classification, object detection, and segmentation.\n",
    "ğŸ—£ï¸ Audio, for tasks like speech recognition and audio classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef1e6c3",
   "metadata": {},
   "source": [
    "# pip install transformers\n",
    "Tokenizers are essential tools in machine learning, especially in natural language processing (NLP). They break down text into smaller units called tokens.\n",
    "\n",
    "These tokens can be words, subwords, or characters.\n",
    "\n",
    "For example, let's take the sentence \"I love apples.\" When we tokenize this sentence, it breaks down into three tokens: \"I,\" \"love,\" and \"apples.\"\n",
    "\n",
    "Tokenization makes it easier for computers to understand and process the text. Itâ€™s used for tasks like translation, sentiment analysis, and all of NLP.\n",
    "\n",
    "AutoTokenizer is a special class in the Huggingface Transformers library. It helps you choose the right tokenizer for your model without knowing the details.\n",
    "\n",
    "Think of it as a smart assistant that knows which tool to use for the job.\n",
    "\n",
    "The AutoTokenizer is easy to use. You donâ€™t have to remember which tokenizer goes with which model. It ensures you use the correct tokenizer for the model, reducing errors and improving consistency.\n",
    "\n",
    "Autotokenizer is flexible. It works with many different models, allowing you to switch models without changing much code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab3d542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "3.8.3\n",
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string,time\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "print(torch.__version__)\n",
    "print(spacy.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05be6bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/georgeli/nltk_data',\n",
       " '/Users/georgeli/anaconda3/nltk_data',\n",
       " '/Users/georgeli/anaconda3/share/nltk_data',\n",
       " '/Users/georgeli/anaconda3/lib/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/local/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/local/lib/nltk_data']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544a5689",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/georgeli/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/georgeli/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/georgeli/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /Users/georgeli/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32128e5d",
   "metadata": {},
   "source": [
    "Look up a word using synsets(); this function has an optional pos argument which lets you constrain the part of speech of the word. ä½¿ç”¨ synsets() æŸ¥æ‰¾å•è¯ï¼›è¿™ä¸ªå‡½æ•°æœ‰ä¸€ä¸ªå¯é€‰çš„ pos å‚æ•°ï¼Œå®ƒå¯ä»¥è®©ä½ é™åˆ¶å•è¯çš„è¯æ€§ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2924bed",
   "metadata": {},
   "source": [
    "WordNet: an NLTK interfaceï¼ˆæ¥å£)for WordNet. \n",
    "WordNet is a lexical databaseï¼ˆè¯æ±‡æ•°æ®åº“ï¼‰ of English. \n",
    "Using synsetsï¼ˆåŒä¹‰è¯é›†ï¼‰, helps find conceptual relationships between words such as hypernymsï¼ˆä¸Šä½è¯ï¼‰, hyponymsï¼ˆä¸‹ä½è¯ï¼‰, synonymsï¼ˆåŒä¹‰è¯ï¼‰, antonymsï¼ˆåä¹‰è¯ï¼‰ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8477025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('happy.a.01'), Synset('felicitous.s.02'), Synset('glad.s.02'), Synset('happy.s.04')]\n",
      "[Synset('chase.v.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# Synset: a set of synonyms that share a common meaning.\n",
    "print(wordnet.synsets('happy'))\n",
    "print(wordnet.synsets('dog', pos=wordnet.VERB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28fd1e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go after with the intent to catch\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.synset('chase.v.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50d6aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dog barked all night\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.synset('dog.n.01').examples()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3e62cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ã„ã¬', 'ã¾ã‚ã—è€…', 'ã‚¹ãƒ‘ã‚¤', 'å›ã—è€…', 'å›è€…', 'å¯†åµ', 'å·¥ä½œå“¡', 'å»»ã—è€…', 'å»»è€…', 'æ¢', 'æ¢ã‚Š', 'çŠ¬', 'ç§˜å¯†æœæŸ»å“¡', 'è«œå ±å“¡', 'è«œè€…', 'é–“è€…', 'é–“è«œ', 'éš å¯†']\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.synset('spy.n.01').lemma_names('jpn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "441edd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['auto', 'automobile', 'machine', 'motorcar'],\n",
       " ['railcar', 'railroad_car', 'railway_car'],\n",
       " ['gondola'],\n",
       " ['elevator_car'],\n",
       " ['cable_car']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The synonyms of a word are returned as a nested list of synonyms of the different senses of the input word in the given language, since these different senses are not mutual synonyms:\n",
    "wordnet.synonyms('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1e400",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08bc4874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Household</td>\n",
       "      <td>Paper Plane Design Framed Wall Hanging Motivat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Household</td>\n",
       "      <td>SAF 'Floral' Framed Painting (Wood, 30 inch x ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Household</td>\n",
       "      <td>SAF 'UV Textured Modern Art Print Framed' Pain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Household</td>\n",
       "      <td>SAF Flower Print Framed Painting (Synthetic, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Household</td>\n",
       "      <td>Incredible Gifts India Wooden Happy Birthday U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     segment                                               text\n",
       "0  Household  Paper Plane Design Framed Wall Hanging Motivat...\n",
       "1  Household  SAF 'Floral' Framed Painting (Wood, 30 inch x ...\n",
       "2  Household  SAF 'UV Textured Modern Art Print Framed' Pain...\n",
       "3  Household  SAF Flower Print Framed Painting (Synthetic, 1...\n",
       "4  Household  Incredible Gifts India Wooden Happy Birthday U..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"ecommerceDataset.csv\",header=None, names=[\"segment\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdf45683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50425, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3322584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "segment\n",
       "Household                 19313\n",
       "Books                     11820\n",
       "Electronics               10621\n",
       "Clothing & Accessories     8671\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.segment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d51e7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'segment'\n",
    "samples_per_class = 600\n",
    "balanced_data = df.groupby(label_column).apply(lambda x: x.sample(n=samples_per_class, random_state=42))\n",
    "balanced_data = balanced_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e372c6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   segment  2400 non-null   object\n",
      " 1   text     2400 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 37.6+ KB\n"
     ]
    }
   ],
   "source": [
    "balanced_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3e87397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, max_length=128):\n",
    "        # Bert tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") #https://huggingface.co/google-bert/bert-base-uncased\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        pattern = re.compile('<.*?>')\n",
    "        text=pattern.sub(r'', text)\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "        text=text.translate(str.maketrans('', '', string.punctuation))\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokenizer = TweetTokenizer()\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        text = ' '.join(tokens)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "        text = ' '.join(tokens)\n",
    "        return text.strip()\n",
    "    def text_to_sequence(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d81b2cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organic Shastra Pav Bhaji Masala 150g Pav bhaji, a popular street food of India, is incomplete without Pav Bhaji masala. It is a blend of spices - red chillies, coriander seeds, cumin seeds, black pepper, cinnamon, clove, black cardamom, dry mango powder, fennel seeds and turmeric powder. Not just in pav bhaji, this aromatic masala can also be used to spice up curries, stir fries, rice preparations etc\n",
      "**************************************************\n",
      "organic shastra pav bhaji masala 150g pav bhaji popular street food india incomplete without pav bhaji masala blend spice red chilli coriander seed cumin seed black pepper cinnamon clove black cardamom dry mango powder fennel seed turmeric powder pav bhaji aromatic masala also used spice curry stir fry rice preparation etc\n"
     ]
    }
   ],
   "source": [
    "prosesor=TextPreprocessor()\n",
    "test=balanced_data['text'][1]\n",
    "print(test)\n",
    "print('*'*50)\n",
    "print(prosesor.clean_text(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "191ea096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, preprocessor=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.preprocessor.text_to_sequence(text)\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'],\n",
    "            'attention_mask': encoding['attention_mask'],\n",
    "            'length': torch.sum(encoding['attention_mask'])\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['label'] = torch.tensor(self.labels[idx])\n",
    "            \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f538c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Books', 'Clothing & Accessories', 'Electronics', 'Household']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1c19c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {category: idx for idx, category in enumerate(categories)}\n",
    "balanced_data['segment'] = balanced_data['segment'].map(category_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15571b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "texts = balanced_data.text.tolist()\n",
    "labels = balanced_data.segment.tolist()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25eee373",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,bidirectional=True)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids,attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "         # Extract the output of the last time step \n",
    "        _,(hidden,_)=self.lstm(embedded)\n",
    "        \n",
    "        # Shape Hidden:(number of layers* direction , batch,  hidden)\n",
    "        hidden_last_layer = hidden[-1,:,:]\n",
    "        hidden_last_layer = self.dropout(hidden_last_layer)\n",
    "\n",
    "        # Linear classifier layer\n",
    "        output = self.classifier(hidden_last_layer)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc2ef8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score,classification_report\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predicted_labels = []\n",
    "    all_true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='eval'):\n",
    "            input_ids= batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            _,predicted_labels=torch.max(outputs, dim =1 )\n",
    "\n",
    "\n",
    "            all_predicted_labels.extend(predicted_labels.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "    # Calculating Metrics using sklearn.metrics\n",
    "    accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n",
    "    f1= f1_score(all_true_labels,all_predicted_labels, average = 'weighted')\n",
    "    \n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83ea57e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 960/960 [00:46<00:00, 20.69it/s, loss=0.0003]\n",
      "eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:02<00:00, 109.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 , Train Loss: 0.9428, Val_Accuracy: 0.7771 Val_F1:0.7789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 960/960 [00:43<00:00, 22.16it/s, loss=0.0016]\n",
      "eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:01<00:00, 138.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 , Train Loss: 0.4076, Val_Accuracy: 0.6917 Val_F1:0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 960/960 [00:44<00:00, 21.66it/s, loss=0.0000]\n",
      "eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:02<00:00, 112.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 , Train Loss: 0.1804, Val_Accuracy: 0.8583 Val_F1:0.8576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 960/960 [00:44<00:00, 21.64it/s, loss=0.0000]\n",
      "eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:01<00:00, 133.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 , Train Loss: 0.0708, Val_Accuracy: 0.8438 Val_F1:0.8422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 960/960 [00:43<00:00, 22.06it/s, loss=0.0000]\n",
      "eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:01<00:00, 134.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 , Train Loss: 0.0387, Val_Accuracy: 0.8771 Val_F1:0.8777\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "train_dataset = SentimentDataset(train_texts, train_labels, preprocessor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "val_dataset = SentimentDataset(val_texts, val_labels, preprocessor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "\n",
    "# Model Instantiation\n",
    "vocab_size=preprocessor.vocab_size\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_labels = 4\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_labels)\n",
    "\n",
    "\n",
    "# --- Training setup ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# --- Training loop ---\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    model.train()  # Put the model in training mode\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader),desc=f\"Epoch : {epoch+1}/{epochs}\")\n",
    "    for idx, batch in progress_bar : \n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "\n",
    "        # Calculating Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        # Backward Pass & optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar loss in training\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item() / (idx + 1) :.4f}'})\n",
    "\n",
    "    avg_loss = total_loss/ len(train_dataloader)\n",
    "\n",
    "\n",
    "    # Evaluate every epoch after training.\n",
    "    val_accuracy,val_f1 = evaluate(model,val_dataloader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} , Train Loss: {avg_loss:.4f}, Val_Accuracy: {val_accuracy:.4f} Val_F1:{val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c24e346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Example\n",
    "def predict_sentiment(text, model, preprocessor, device):\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = preprocessor.text_to_sequence(text)\n",
    "    input_ids = encoding['input_ids'].unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=-1)\n",
    "\n",
    "\n",
    "    predicted_label = torch.argmax(probabilities).item()\n",
    "    return predicted_label, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c84b254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor([[0.8468, 0.0015, 0.0070, 0.1446]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(test, model, preprocessor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc57828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num={\n",
    "    'Books':0,\n",
    "    'Clothing & Accessories':1,\n",
    "    'Electronics':2,\n",
    "    'Household':3\n",
    "}\n",
    "df['segment'] = df['segment'].map(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "094c414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_row = df.sample(n=1)\n",
    "random_text = random_row['text'].values[0]\n",
    "random_Sentment = random_row['segment'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8368fd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, tensor([[1.2434e-04, 1.2626e-04, 7.5909e-04, 9.9899e-01]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(random_text, model, preprocessor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "356fef1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_Sentment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91b16c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
